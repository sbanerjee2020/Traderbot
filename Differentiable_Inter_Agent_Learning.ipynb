{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Differentiable Inter Agent Learning",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sbanerjee2020/Traderbot/blob/master/Differentiable_Inter_Agent_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "RwvVOYFfqr2O",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Differentiable Inter Agent Learning\n",
        "Implementation of Differentiable Inter Agent Learning, to learn communication in multi-agent systems, on the Switch Riddle Problem. \n",
        "\n",
        "Original Paper: [Foerster et al., 2016](https://arxiv.org/abs/1605.06676)\n",
        "\n",
        "Official Torch (Lua) Implementation: [iassael/learning-to-communicate](https://github.com/iassael/learning-to-communicate)\n",
        "\n",
        "Adapted from: [minqi/learning-to-communicate-pytorch](https://github.com/minqi/learning-to-communicate-pytorch)"
      ]
    },
    {
      "metadata": {
        "id": "VPhlxEYYc6ey",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import numpy as np\n",
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn import functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.utils import clip_grad_norm_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oWweN0zWXp8G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Dictionary with items accessible as attributes (through '.' operator)\n",
        "class DotDic(dict):\n",
        "\t__getattr__ = dict.get\n",
        "\t__setattr__ = dict.__setitem__\n",
        "\t__delattr__ = dict.__delitem__\n",
        "\n",
        "\tdef __deepcopy__(self, memo=None):\n",
        "\t\treturn DotDic(copy.deepcopy(dict(self), memo=memo))\n",
        "\n",
        "# To reset the weights of layers in nn.Sequential model.\n",
        "def weight_reset(m):\n",
        "  if isinstance(m, nn.BatchNorm1d) or isinstance(m, nn.Linear):\n",
        "    m.reset_parameters()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z2iG0tdx--4D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Environment\n",
        "We will test our DIAL implementation on the [Switch Riddle](https://medium.com/i-math/100-prisoners-and-a-light-bulb-573426272f4c) problem with 3 prisoners (agents). \n",
        "The environment works as follows:\n",
        "* The agents are assigned to each time step in an epsiode randomly. (This corresponds to the warden calling a random prisoner to the central living room)\n",
        "* The agent can either do _`nothing`_ or _`tell`_. (These are the environment actions available to the agent) If the agent tells before all the prisoners have been to the living room once, all the agents get a reward of -1, else if all the agents have been to the central living room at least once, all agents get a reward of +1. (Reward of -1 corresponds to all prisoners dying and reward of +1 corresponds to all prisoners making it out alive)\n",
        "* The agent can also communicate with the other agents through a 1-bit communication channel. (This bit corresponds to the state of the bulb) This value is a real value during training but takes only discrete binary values during test.  \n",
        "\n",
        "The environment constucts the episode and assigns  random agents at each timestep and the agent can take action(along with sending the message)."
      ]
    },
    {
      "metadata": {
        "id": "cf5DxCae9N40",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SwitchGame:\n",
        "    def __init__(self, opt):\n",
        "        \"\"\"\n",
        "        Initializes the Switch Game with given parameters.\n",
        "        \"\"\"     \n",
        "        self.game_actions = DotDic({\n",
        "            'NOTHING': 1,\n",
        "            'TELL': 2\n",
        "        })\n",
        "\n",
        "        self.game_states = DotDic({\n",
        "            'OUTSIDE': 0,\n",
        "            'INSIDE': 1,\n",
        "        })\n",
        "\n",
        "        self.opt = opt\n",
        "\n",
        "        # Set game defaults\n",
        "        opt_game_default = DotDic({\n",
        "            'game_action_space': 2,\n",
        "            'game_reward_shift': 0,\n",
        "            'game_comm_bits': 1,\n",
        "            'game_comm_sigma': 2\n",
        "        })\n",
        "        for k in opt_game_default:\n",
        "            if k not in self.opt:\n",
        "                self.opt[k] = opt_game_default[k]\n",
        "\n",
        "        self.opt.nsteps = 4 * self.opt.game_nagents - 6\n",
        "\n",
        "        self.reward_all_live = 1\n",
        "        self.reward_all_die = -1\n",
        "\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Resets the environment for the next episode and sets up the agent sequence for the next episode. \n",
        "        \"\"\"\n",
        "        # Step count\n",
        "        self.step_count = 0\n",
        "\n",
        "        # Rewards\n",
        "        self.reward = torch.zeros(self.opt.bs, self.opt.game_nagents)\n",
        "\n",
        "        # Who has been in the room?\n",
        "        self.has_been = torch.zeros(self.opt.bs, self.opt.nsteps, self.opt.game_nagents)\n",
        "\n",
        "        # Terminal state\n",
        "        self.terminal = torch.zeros(self.opt.bs, dtype=torch.long)\n",
        "\n",
        "        # Active agent\n",
        "        self.active_agent = torch.zeros(self.opt.bs, self.opt.nsteps, dtype=torch.long) # 1-indexed agents\n",
        "        for b in range(self.opt.bs):\n",
        "            for step in range(self.opt.nsteps):\n",
        "                agent_id = 1 + np.random.randint(self.opt.game_nagents)\n",
        "                self.active_agent[b][step] = agent_id\n",
        "                self.has_been[b][step][agent_id - 1] = 1\n",
        "\n",
        "        return self\n",
        "\n",
        "    def get_action_range(self, step, agent_id):\n",
        "        \"\"\"\n",
        "        Return 1-indexed indices into Q vector for valid actions and communications (so 0 represents no-op)\n",
        "        \"\"\"\n",
        "        opt = self.opt\n",
        "        action_dtype = torch.long\n",
        "        action_range = torch.zeros((self.opt.bs, 2), dtype=action_dtype)\n",
        "        comm_range = torch.zeros((self.opt.bs, 2), dtype=action_dtype)\n",
        "        for b in range(self.opt.bs): \n",
        "            if self.active_agent[b][step] == agent_id:\n",
        "                action_range[b] = torch.tensor([1, opt.game_action_space], dtype=action_dtype)\n",
        "                comm_range[b] = torch.tensor(\n",
        "                    [opt.game_action_space + 1, opt.game_action_space_total], dtype=action_dtype)\n",
        "            else:\n",
        "                action_range[b] = torch.tensor([1, 1], dtype=action_dtype)\n",
        "\n",
        "        return action_range, comm_range\n",
        "\n",
        "    def get_comm_limited(self, step, agent_id):\n",
        "        \"\"\"\n",
        "        Returns the possible communication options.\n",
        "        \"\"\"\n",
        "        if self.opt.game_comm_limited:\n",
        "            comm_lim = torch.zeros(self.opt.bs, dtype=torch.long)\n",
        "            for b in range(self.opt.bs):\n",
        "                if step > 0 and agent_id == self.active_agent[b][step]:\n",
        "                    comm_lim[b] = self.active_agent[b][step - 1]\n",
        "            return comm_lim\n",
        "        return None\n",
        "\n",
        "    def get_reward(self, a_t):\n",
        "        \"\"\"\n",
        "        Returns the reward for action a_t taken by current agent in state a_t\n",
        "        \"\"\"\n",
        "        for b in range(self.opt.bs):\n",
        "            active_agent_idx = self.active_agent[b][self.step_count].item() - 1\n",
        "            if a_t[b][active_agent_idx].item() == self.game_actions.TELL and not self.terminal[b].item():\n",
        "                has_been = self.has_been[b][:self.step_count + 1].sum(0).gt(0).sum(0).item()\n",
        "                if has_been == self.opt.game_nagents:\n",
        "                    self.reward[b] = self.reward_all_live\n",
        "                else:\n",
        "                    self.reward[b] = self.reward_all_die\n",
        "                self.terminal[b] = 1\n",
        "            elif self.step_count == self.opt.nsteps - 1 and not self.terminal[b]:\n",
        "                self.terminal[b] = 1\n",
        "\n",
        "        return self.reward.clone(), self.terminal.clone()\n",
        "\n",
        "    def step(self, a_t):\n",
        "        \"\"\"\n",
        "        Executes action a_t by current agent and returns reward and terminal status.\n",
        "        \"\"\"\n",
        "        reward, terminal = self.get_reward(a_t)\n",
        "        self.step_count += 1\n",
        "\n",
        "        return reward, terminal\n",
        "\n",
        "    def get_state(self):\n",
        "        \"\"\"\n",
        "        Returns the current game state.\n",
        "        \"\"\"\n",
        "        state = torch.zeros(self.opt.bs, self.opt.game_nagents, dtype=torch.long)\n",
        "\n",
        "        # Get the state of the game\n",
        "        for b in range(self.opt.bs):\n",
        "            for a in range(1, self.opt.game_nagents + 1):\n",
        "                if self.active_agent[b][self.step_count] == a:\n",
        "                    state[b][a - 1] = self.game_states.INSIDE\n",
        "\n",
        "        return state\n",
        "\n",
        "    def oracle_strategy_reward(self, steps):\n",
        "        \"\"\"\n",
        "        Returns the episodic return for the optimal strategy, to normalize the rewards.\n",
        "        \"\"\"\n",
        "        reward = torch.zeros(self.opt.bs)\n",
        "        for b in range(self.opt.bs):\n",
        "            has_been = self.has_been[b][:self.opt.nsteps].sum(0).gt(0).sum().item()\n",
        "            if has_been == self.opt.game_nagents:\n",
        "                reward[b] = self.reward_all_live\n",
        "\n",
        "        return reward\n",
        "        \n",
        "    def get_stats(self, steps):\n",
        "        stats = DotDic({})\n",
        "        stats.oracle_reward = self.oracle_strategy_reward(steps)\n",
        "        return stats\n",
        "\n",
        "    def describe_game(self, b=0):\n",
        "        print('has been:', self.has_been[b])\n",
        "        print('num has been:', self.has_been[b].sum(0).gt(0).sum().item())\n",
        "        print('active agents: ', self.active_agent[b])\n",
        "        print('reward:', self.reward[b])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "84iWm7_NDpzB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Discretise/Regularise unit\n",
        "In the DIAL approach, the agents are allowed to send real-valued messages during the learning phase, but can communicate only in discrete bits during test. This unit implements this functionality. Sigmoid is used for the the messages in the learning phase.\n",
        "\n",
        "$DRU(m^a_t) = Logistic(N (m^a_t, σ))$ during training\n",
        "\n",
        "$DRU(m^a_t) = 1\\{m_t^a > 0\\}$"
      ]
    },
    {
      "metadata": {
        "id": "1aP5vR9Ce1Q_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DRU:\n",
        "    def __init__(self, sigma, comm_narrow=True, hard=False):\n",
        "        self.sigma = sigma\n",
        "        self.comm_narrow = comm_narrow\n",
        "        self.hard = hard\n",
        "\n",
        "    def regularize(self, m):\n",
        "        \"\"\"\n",
        "        Returns the regularized value of message `m` during training.\n",
        "        \"\"\"\n",
        "        m_reg = m + torch.randn(m.size()) * self.sigma\n",
        "        if self.comm_narrow:\n",
        "            m_reg = torch.sigmoid(m_reg)\n",
        "        else:\n",
        "            m_reg = torch.softmax(m_reg, 0)\n",
        "        return m_reg\n",
        "\n",
        "    def discretize(self, m):\n",
        "        \"\"\"\n",
        "        Returns the discretized value of message `m` during execution.\n",
        "        \"\"\"\n",
        "        if self.hard:\n",
        "            if self.comm_narrow:\n",
        "                return (m.gt(0.5).float() - 0.5).sign().float()\n",
        "            else:\n",
        "                m_ = torch.zeros_like(m)\n",
        "                if m.dim() == 1:      \n",
        "                    _, idx = m.max(0)\n",
        "                    m_[idx] = 1.\n",
        "                elif m.dim() == 2:      \n",
        "                    _, idx = m.max(1)\n",
        "                    for b in range(idx.size(0)):\n",
        "                        m_[b, idx[b]] = 1.\n",
        "                else:\n",
        "                    raise ValueError('Wrong message shape: {}'.format(m.size()))\n",
        "                return m_\n",
        "        else:\n",
        "            scale = 2 * 20\n",
        "            if self.comm_narrow:\n",
        "                return torch.sigmoid((m.gt(0.5).float() - 0.5) * scale)\n",
        "            else:\n",
        "                return torch.softmax(m * scale, -1)\n",
        "\n",
        "    def forward(self, m, train_mode):\n",
        "        if train_mode:\n",
        "            return self.regularize(m)\n",
        "        else:\n",
        "            return self.discretize(m)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G5e0IeqmIJJj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## CNet\n",
        "CNet is a neural network architecture for the DIAL approach. It takes as input the current state, messages, previous action and the agent id and outputs the the Q-values and the hidden state of the RNN.\n",
        "\n",
        "$Q_u(o^a_t, m^{a'}_{t−1}, h^a_{t−1}, u^a_{t−1}, m^a_{t−1}, a, u^a_t)$\n",
        "\n",
        "The environment action and message can be decided from the q-values by the epsilon greedy policy and the DRU respectively."
      ]
    },
    {
      "metadata": {
        "id": "-iA-nlp6dqJ3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class CNet(nn.Module):\n",
        "  def __init__(self, opts):\n",
        "    \"\"\"\n",
        "    Initializes the CNet model\n",
        "    \"\"\"\n",
        "    super(CNet, self).__init__()\n",
        "    self.opts = opts\n",
        "    self.comm_size = opts['game_comm_bits']\n",
        "    self.init_param_range = (-0.08, 0.08)\n",
        "    \n",
        "    ## Lookup tables for the state, action and previous action.\n",
        "    self.action_lookup = nn.Embedding(opts['game_nagents'], opts['rnn_size'])\n",
        "    self.state_lookup = nn.Embedding(2, opts['rnn_size'])\n",
        "    self.prev_action_lookup = nn.Embedding(opts['game_action_space_total'], opts['rnn_size'])\n",
        "    \n",
        "    # Single layer MLP(with batch normalization for improved performance) for producing embeddings for messages.\n",
        "    self.message = nn.Sequential(\n",
        "      nn.BatchNorm1d(self.comm_size),\n",
        "      nn.Linear(self.comm_size, opts['rnn_size']),\n",
        "      nn.ReLU(inplace=True)\n",
        "    )\n",
        "    \n",
        "    # RNN to approximate the agent’s action-observation history.\n",
        "    self.rnn = nn.GRU(input_size=opts['rnn_size'], hidden_size=opts['rnn_size'], num_layers=2, batch_first=True)\n",
        "    \n",
        "    # 2 layer MLP with batch normalization, for producing output from RNN top layer.\n",
        "    self.output = nn.Sequential(\n",
        "      nn.Linear(opts['rnn_size'], opts['rnn_size']),\n",
        "      nn.BatchNorm1d(opts['rnn_size']),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(opts['rnn_size'], opts['game_action_space_total'])\n",
        "    )\n",
        "    \n",
        "  def get_params(self):\n",
        "    return list(self.parameters())\n",
        "\n",
        "  def reset_parameters(self):\n",
        "    \"\"\"\n",
        "    Reset all model parameters\n",
        "    \"\"\"\n",
        "    self.rnn.reset_parameters()\n",
        "    self.action_lookup.reset_parameters()\n",
        "    self.state_lookup.reset_parameters()\n",
        "    self.prev_action_lookup.reset_parameters()\n",
        "    self.message.apply(weight_reset)\n",
        "    self.output.apply(weight_reset)\n",
        "    for p in self.rnn.parameters():\n",
        "      p.data.uniform_(*self.init_param_range)\n",
        "\n",
        "  def forward(self, state, messages, hidden, prev_action, agent):\n",
        "    \"\"\"\n",
        "    Returns the q-values and hidden state for the given step parameters\n",
        "    \"\"\"\n",
        "    state = Variable(torch.LongTensor(state))\n",
        "    hidden = Variable(torch.FloatTensor(hidden))\n",
        "    prev_action = Variable(torch.LongTensor(prev_action))\n",
        "    agent = Variable(torch.LongTensor(agent))\n",
        "\n",
        "    # Produce embeddings for rnn from input parameters\n",
        "    z_a = self.action_lookup(agent)\n",
        "    z_o = self.state_lookup(state)\n",
        "    z_u = self.prev_action_lookup(prev_action)\n",
        "    z_m = self.message(messages.view(-1, self.comm_size))\n",
        "    \n",
        "    # Add the input embeddings to calculate final RNN input.\n",
        "    z = z_a + z_o + z_u + z_m\n",
        "    z = z.unsqueeze(1)\n",
        "\n",
        "    rnn_out, h = self.rnn(z, hidden)\n",
        "    # Produce final CNet output q-values from GRU output.\n",
        "    out = self.output(rnn_out[:, -1, :].squeeze())\n",
        "\n",
        "    return h, out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4281O_yEVB3i",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Agent"
      ]
    },
    {
      "metadata": {
        "id": "YnoO2UA5L3pk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "  def __init__(self, opts, game, model, target, agent_no):\n",
        "    \"\"\"\n",
        "    Initializes the agent(with id=agent_no) with given model and target_model\n",
        "    \"\"\"\n",
        "    self.game = game\n",
        "    self.opts = opts\n",
        "    self.model = model\n",
        "    self.model_target = target\n",
        "    self.id = agent_no\n",
        "    \n",
        "    # Make target model not trainable\n",
        "    for param in target.parameters():\n",
        "      param.requires_grad = False\n",
        "    \n",
        "    self.episodes = 0\n",
        "    self.dru = DRU(opts['game_comm_sigma'])\n",
        "    self.optimizer = optim.RMSprop(params=model.get_params(), lr=opts['lr'], momentum=opts['momentum'])\n",
        "    \n",
        "  def reset(self):\n",
        "    \"\"\"\n",
        "    Resets the agent parameters\n",
        "    \"\"\"\n",
        "    self.model.reset_parameters()\n",
        "    self.model_target.reset_parameters()\n",
        "    self.episodes = 0\n",
        "    \n",
        "  def _eps_flip(self, eps):\n",
        "    return np.random.rand(self.opts['bs']) < eps\n",
        "\n",
        "  def _random_choice(self, items):\n",
        "    return torch.from_numpy(np.random.choice(items, 1)).item()\n",
        "  \n",
        "  def select(self, step, q, eps=0, target = False, train=False):\n",
        "    \"\"\"\n",
        "    Returns the (action, communication) for the current step.\n",
        "    \"\"\"\n",
        "    if not train:\n",
        "      eps = 0 # Pick greedily during test\n",
        "    \n",
        "    opts = self.opts\n",
        "    \n",
        "    # Get the action range and communication range for the agent for the current time step.\n",
        "    action_range, comm_range = self.game.get_action_range(step, self.id)\n",
        "    \n",
        "    action = torch.zeros(opts['bs'], dtype=torch.long)\n",
        "    action_value = torch.zeros(opts['bs'])\n",
        "    comm_vector = torch.zeros(opts['bs'], opts['game_comm_bits'])\n",
        "    \n",
        "    \n",
        "    select_random_a = self._eps_flip(eps)\n",
        "    for b in range(opts['bs']):\n",
        "      q_a_range = range(0, opts['game_action_space'])\n",
        "      a_range = range(action_range[b, 0].item() - 1, action_range[b, 1].item())\n",
        "      if select_random_a[b]:\n",
        "        # select action randomly (to explore the state space)\n",
        "        action[b] = self._random_choice(a_range)\n",
        "        action_value[b] = q[b, action[b]]\n",
        "      else:\n",
        "        action_value[b], action[b] = q[b, a_range].max(0) # select action greedily\n",
        "      action[b] = action[b] + 1\n",
        "      \n",
        "      q_c_range = range(opts['game_action_space'], opts['game_action_space_total'])\n",
        "      if comm_range[b, 1] > 0:\n",
        "        # if the agent can communicate for the given time step\n",
        "        c_range = range(comm_range[b, 0].item() - 1, comm_range[b, 1].item())\n",
        "        # real-valued message from DRU based on q-values\n",
        "        comm_vector[b] = self.dru.forward(q[b, q_c_range], train_mode=train)\n",
        "    return (action, action_value), comm_vector\n",
        "  \n",
        "  def get_loss(self, episode):\n",
        "    \"\"\"\n",
        "    Returns episodic loss for the given episodes.\n",
        "    \"\"\"\n",
        "    opts = self.opts\n",
        "    total_loss = torch.zeros(opts['bs'])\n",
        "    for b in range(opts['bs']):\n",
        "      b_steps = episode.steps[b].item()\n",
        "      for step in range(b_steps):\n",
        "        record = episode.step_records[step]\n",
        "        for i in range(opts['game_nagents']):\n",
        "          td_action = 0\n",
        "          r_t = record.r_t[b][i]\n",
        "          q_a_t = record.q_a_t[b][i]\n",
        "          \n",
        "          # Calculate td loss for environment action\n",
        "          if record.a_t[b][i].item() > 0:\n",
        "            if record.terminal[b].item() > 0:\n",
        "              td_action = r_t - q_a_t\n",
        "            else:\n",
        "              next_record = episode.step_records[step + 1]\n",
        "              q_next_max = next_record.q_a_max_t[b][i]\n",
        "              td_action = r_t = opts['gamma'] * q_next_max - q_a_t\n",
        "          \n",
        "          loss_t = td_action ** 2\n",
        "          total_loss[b] = total_loss[b] + loss_t\n",
        "    loss = total_loss.sum()\n",
        "    return loss / (opts['bs'] * opts['game_nagents'])\n",
        "  \n",
        "  def update(self, episode):\n",
        "    \"\"\"\n",
        "    Updates model parameters for given episode batch\n",
        "    \"\"\"\n",
        "    self.optimizer.zero_grad()\n",
        "    loss = self.get_loss(episode)\n",
        "    loss.backward()\n",
        "    # Clip gradients for stable training\n",
        "    clip_grad_norm_(parameters=self.model.get_params(), max_norm=10)\n",
        "    self.optimizer.step()\n",
        "    self.episodes += 1\n",
        "    \n",
        "    # Update target model\n",
        "    if self.episodes % self.opts['step_target'] == 0:\n",
        "      self.model_target.load_state_dict(self.model.state_dict())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I2JTazxPXuBV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##  Training"
      ]
    },
    {
      "metadata": {
        "id": "19t10sGx_hrK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Arena:\n",
        "  def __init__(self, opt, game):\n",
        "    self.opt = opt\n",
        "    self.game = game\n",
        "    self.eps = opt.eps\n",
        "\n",
        "  def create_episode(self):\n",
        "    \"\"\"\n",
        "    Returns an episode dictionary to maintain current episode details\n",
        "    \"\"\"\n",
        "    opt = self.opt\n",
        "    episode = DotDic({})\n",
        "    episode.steps = torch.zeros(opt.bs).int()\n",
        "    episode.ended = torch.zeros(opt.bs).int()\n",
        "    episode.r = torch.zeros(opt.bs, opt.game_nagents).float()\n",
        "    episode.step_records = []\n",
        "\n",
        "    return episode\n",
        "\n",
        "  def create_step_record(self):\n",
        "    \"\"\"\n",
        "    Returns an empty step record to store the data from each step in the episode\n",
        "    \"\"\"\n",
        "    opt = self.opt\n",
        "    record = DotDic({})\n",
        "    record.s_t = None\n",
        "    record.r_t = torch.zeros(opt.bs, opt.game_nagents)\n",
        "    record.terminal = torch.zeros(opt.bs)\n",
        "\n",
        "    record.agent_inputs = []\n",
        "    record.a_t = torch.zeros(opt.bs, opt.game_nagents, dtype=torch.long)\n",
        "    record.comm = torch.zeros(opt.bs, opt.game_nagents, opt.game_comm_bits)\n",
        "    record.comm_target = record.comm.clone()\n",
        "    \n",
        "    record.hidden = torch.zeros(opt.game_nagents, 2, opt.bs, opt.rnn_size)\n",
        "    record.hidden_target = torch.zeros(opt.game_nagents, 2, opt.bs, opt.rnn_size)\n",
        "\n",
        "    record.q_a_t = torch.zeros(opt.bs, opt.game_nagents)\n",
        "    record.q_a_max_t = torch.zeros(opt.bs, opt.game_nagents)\n",
        "\n",
        "    return record\n",
        "\n",
        "  def run_episode(self, agents, train_mode=False):\n",
        "    \"\"\"\n",
        "    Runs one batch of episodes for the given agents.\n",
        "    \"\"\"\n",
        "    opt = self.opt\n",
        "    game = self.game\n",
        "    game.reset()\n",
        "    self.eps = self.eps * opt.eps_decay\n",
        "\n",
        "    step = 0\n",
        "    episode = self.create_episode()\n",
        "    s_t = game.get_state()\n",
        "    # Intialize step record\n",
        "    episode.step_records.append(self.create_step_record())\n",
        "    episode.step_records[-1].s_t = s_t\n",
        "    episode_steps = train_mode and opt.nsteps + 1 or opt.nsteps\n",
        "    while step < episode_steps and episode.ended.sum() < opt.bs:\n",
        "      # Run through the episode\n",
        "      episode.step_records.append(self.create_step_record())\n",
        "\n",
        "      for i in range(1, opt.game_nagents + 1):\n",
        "        agent = agents[i]\n",
        "        agent_idx = i - 1\n",
        "        \n",
        "        # Retrieve model inputs from the records\n",
        "        comm = episode.step_records[step].comm.clone()\n",
        "        comm_limited = self.game.get_comm_limited(step, agent.id)\n",
        "        if comm_limited is not None:\n",
        "          comm_lim = torch.zeros(opt.bs, 1, opt.game_comm_bits)\n",
        "          for b in range(opt.bs):\n",
        "            if comm_limited[b].item() > 0:\n",
        "              comm_lim[b] = comm[b][comm_limited[b] - 1]\n",
        "          comm = comm_lim\n",
        "        else:\n",
        "          comm[:, agent_idx].zero_()\n",
        "        prev_action = torch.ones(opt.bs, dtype=torch.long)\n",
        "        if not opt.model_dial:\n",
        "          prev_message = torch.ones(opt.bs, dtype=torch.long)\n",
        "        for b in range(opt.bs):\n",
        "          if step > 0 and episode.step_records[step - 1].a_t[b, agent_idx] > 0:\n",
        "            prev_action[b] = episode.step_records[step - 1].a_t[b, agent_idx]\n",
        "        batch_agent_index = torch.zeros(opt.bs, dtype=torch.long).fill_(agent_idx)\n",
        "\n",
        "        agent_inputs = {\n",
        "          'state': episode.step_records[step].s_t[:, agent_idx],\n",
        "          'messages': comm,\n",
        "          'hidden': episode.step_records[step].hidden[agent_idx, :],\n",
        "          'prev_action': prev_action,\n",
        "          'agent': batch_agent_index\n",
        "        }\n",
        "        episode.step_records[step].agent_inputs.append(agent_inputs)\n",
        "        \n",
        "        # Get Q-values from CNet\n",
        "        hidden_t, q_t = agent.model(**agent_inputs)\n",
        "        episode.step_records[step + 1].hidden[agent_idx] = hidden_t.squeeze()\n",
        "        # Pick actions based on q-values\n",
        "        (action, action_value), comm_vector = agent.select(step, q_t, eps=self.eps, train=train_mode)\n",
        "\n",
        "        episode.step_records[step].a_t[:, agent_idx] = action\n",
        "        episode.step_records[step].q_a_t[:, agent_idx] = action_value\n",
        "        episode.step_records[step + 1].comm[:, agent_idx] = comm_vector\n",
        "\n",
        "      a_t = episode.step_records[step].a_t\n",
        "      episode.step_records[step].r_t, episode.step_records[step].terminal = self.game.step(a_t)\n",
        "\n",
        "      # Update episode record rewards\n",
        "      if step < opt.nsteps:\n",
        "        for b in range(opt.bs):\n",
        "          if not episode.ended[b]:\n",
        "            episode.steps[b] = episode.steps[b] + 1\n",
        "            episode.r[b] = episode.r[b] + episode.step_records[step].r_t[b]\n",
        "\n",
        "          if episode.step_records[step].terminal[b]:\n",
        "            episode.ended[b] = 1\n",
        "\n",
        "      # Update target network during training\n",
        "      if train_mode:\n",
        "        for i in range(1, opt.game_nagents + 1):\n",
        "          agent_target = agents[i]\n",
        "          agent_idx = i - 1\n",
        "\n",
        "          agent_inputs = episode.step_records[step].agent_inputs[agent_idx]\n",
        "          comm_target = agent_inputs.get('messages', None)\n",
        "\n",
        "          comm_target = episode.step_records[step].comm_target.clone()\n",
        "          comm_limited = self.game.get_comm_limited(step, agent.id)\n",
        "          if comm_limited is not None:\n",
        "            comm_lim = torch.zeros(opt.bs, 1, opt.game_comm_bits)\n",
        "            for b in range(opt.bs):\n",
        "              if comm_limited[b].item() > 0:\n",
        "                comm_lim[b] = comm_target[b][comm_limited[b] - 1]\n",
        "            comm_target = comm_lim\n",
        "          else:\n",
        "            comm_target[:, agent_idx].zero_()\n",
        "\n",
        "          agent_target_inputs = copy.copy(agent_inputs)\n",
        "          agent_target_inputs['messages'] = Variable(comm_target)\n",
        "          agent_target_inputs['hidden'] = episode.step_records[step].hidden_target[agent_idx, :]\n",
        "          hidden_target_t, q_target_t = agent_target.model_target(**agent_target_inputs)\n",
        "          episode.step_records[step + 1].hidden_target[agent_idx] = hidden_target_t.squeeze()\n",
        "\n",
        "          (action, action_value), comm_vector = agent_target.select(step, q_target_t, eps=0, target=True, train=True)\n",
        "\n",
        "          episode.step_records[step].q_a_max_t[:, agent_idx] = action_value\n",
        "          episode.step_records[step + 1].comm_target[:, agent_idx] = comm_vector\n",
        "\n",
        "      step = step + 1\n",
        "      if episode.ended.sum().item() < opt.bs:\n",
        "        episode.step_records[step].s_t = self.game.get_state()\n",
        "\n",
        "    episode.game_stats = self.game.get_stats(episode.steps)\n",
        "\n",
        "    return episode\n",
        "\n",
        "  def average_reward(self, episode, normalized=True):\n",
        "    \"\"\"\n",
        "    Returns the normalized average reward for the episode.\n",
        "    \"\"\"\n",
        "    reward = episode.r.sum()/(self.opt.bs * self.opt.game_nagents)\n",
        "    if normalized:\n",
        "      oracle_reward = episode.game_stats.oracle_reward.sum()/self.opt.bs\n",
        "      if reward == oracle_reward:\n",
        "        reward = 1\n",
        "      elif oracle_reward == 0:\n",
        "        reward = 0\n",
        "      else:\n",
        "        reward = reward/oracle_reward\n",
        "    return float(reward)\n",
        "\n",
        "  def train(self, agents, reset=True, verbose=False, test_callback=None):\n",
        "    \"\"\"\n",
        "    Trains the agents \n",
        "    \"\"\"\n",
        "    opt = self.opt\n",
        "    if reset:\n",
        "      for agent in agents[1:]:\n",
        "        agent.reset()\n",
        "\n",
        "    rewards = []\n",
        "    for e in range(opt.nepisodes):\n",
        "      episode = self.run_episode(agents, train_mode=True)\n",
        "      norm_r = self.average_reward(episode)\n",
        "      if verbose:\n",
        "        print('train epoch:', e, 'avg steps:', episode.steps.float().mean().item(), 'avg reward:', norm_r)\n",
        "      agents[1].update(episode)\n",
        "\n",
        "      if e % opt.step_test == 0:\n",
        "        episode = self.run_episode(agents, train_mode=False)\n",
        "        norm_r = self.average_reward(episode)\n",
        "        rewards.append(norm_r)\n",
        "        print('TEST EPOCH:', e, 'avg steps:', episode.steps.float().mean().item(), 'avg reward:', norm_r)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n8sclGvLs4Tt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Hyperparameters\n",
        "* game_nagents: Number of agents interacting in the environment.\n",
        "* game_action_space: The action space for each agent, i.e. actions that each agent can take.\n",
        "* game_comm_bits: Number of available bits for communication among agents. (Note that this is only only for test phase, where agents communicate in discrete bits.)\n",
        "* game_action_space_total: The total action space for the agent, which includes actions it can take along with the communication bits. Essentially sending messages is another action in itself.\n",
        "* game_comm_limited: This is to allow only one agent to communicate at every time step. \n",
        "* gamma: Discount factor for Q-Learning.\n",
        "* rnn_size: Number of units in RNN (Specifically GRU in this implementation)\n",
        "* bs: Batch Size\n",
        "* lr, momentum: Learning rate and momentum for RMS Optimizer. \n",
        "* eps: Epsilon parameter for epsilon-greedy policy.\n",
        "* eps_decay: Epsilon decay for epsilon-greedy policy.\n",
        "* nsteps: Number of steps in each episode.\n",
        "* nepisodes: Total Number of episodes for training \n",
        "* step_test: Number of training epochs in between each test.\n",
        "* step_target: Number of training epochs before updating the target network.\n"
      ]
    },
    {
      "metadata": {
        "id": "S_7p98VsevO5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "opts = {  \n",
        "   \"game_nagents\":3,\n",
        "   \"game_action_space\":2,\n",
        "   \"game_action_space_total\": 3,\n",
        "   \"game_comm_limited\": True,\n",
        "   \"game_comm_bits\":1,\n",
        "   \"game_comm_sigma\":2,\n",
        "   \"nsteps\":6,\n",
        "   \"gamma\":1,\n",
        "   \"rnn_size\":128,\n",
        "   \"bs\":32,\n",
        "   \"lr\":0.0005,\n",
        "   \"momentum\":0.05,\n",
        "   \"eps\":0.05,\n",
        "   \"nepisodes\":5001,\n",
        "   \"step_test\":10,\n",
        "   \"step_target\":100,\n",
        "   \"eps_decay\": 1.0\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JbtWAN8TUFnq",
        "colab_type": "code",
        "outputId": "29e0ade6-f3ee-4bda-a4a3-a160ddf654aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1948
        }
      },
      "cell_type": "code",
      "source": [
        "game = SwitchGame(DotDic(opts))\n",
        "cnet = CNet(opts)\n",
        "cnet_target = copy.deepcopy(cnet)\n",
        "agents = [None]\n",
        "for i in range(1, opts['game_nagents'] + 1):\n",
        "  agents.append(Agent(DotDic(opts), game=game, model=cnet, target=cnet_target, agent_no=i))\n",
        "\n",
        "arena = Arena(DotDic(opts), game)\n",
        "arena.train(agents)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TEST EPOCH: 0 avg steps: 5.6875 avg reward: 0.2800000011920929\n",
            "TEST EPOCH: 10 avg steps: 3.75 avg reward: -0.09090909361839294\n",
            "TEST EPOCH: 20 avg steps: 4.1875 avg reward: 0.11538461595773697\n",
            "TEST EPOCH: 30 avg steps: 4.96875 avg reward: 0.2916666567325592\n",
            "TEST EPOCH: 40 avg steps: 4.09375 avg reward: 0.1304347813129425\n",
            "TEST EPOCH: 50 avg steps: 4.09375 avg reward: 0.1666666716337204\n",
            "TEST EPOCH: 60 avg steps: 4.375 avg reward: 0.0\n",
            "TEST EPOCH: 70 avg steps: 4.1875 avg reward: 0.5\n",
            "TEST EPOCH: 80 avg steps: 4.53125 avg reward: 0.3333333432674408\n",
            "TEST EPOCH: 90 avg steps: 3.59375 avg reward: -0.375\n",
            "TEST EPOCH: 100 avg steps: 4.375 avg reward: 0.5384615659713745\n",
            "TEST EPOCH: 110 avg steps: 4.28125 avg reward: 0.7083333134651184\n",
            "TEST EPOCH: 120 avg steps: 4.28125 avg reward: 0.5416666865348816\n",
            "TEST EPOCH: 130 avg steps: 4.71875 avg reward: 0.7727272510528564\n",
            "TEST EPOCH: 140 avg steps: 4.5 avg reward: 0.692307710647583\n",
            "TEST EPOCH: 150 avg steps: 4.5625 avg reward: 0.8799999952316284\n",
            "TEST EPOCH: 160 avg steps: 4.84375 avg reward: 0.7200000286102295\n",
            "TEST EPOCH: 170 avg steps: 4.46875 avg reward: 0.8965517282485962\n",
            "TEST EPOCH: 180 avg steps: 4.5 avg reward: 0.9130434989929199\n",
            "TEST EPOCH: 190 avg steps: 4.75 avg reward: 1.0\n",
            "TEST EPOCH: 200 avg steps: 4.78125 avg reward: 0.8518518805503845\n",
            "TEST EPOCH: 210 avg steps: 5.15625 avg reward: 0.9047619104385376\n",
            "TEST EPOCH: 220 avg steps: 4.28125 avg reward: 0.9629629850387573\n",
            "TEST EPOCH: 230 avg steps: 4.4375 avg reward: 0.8846153616905212\n",
            "TEST EPOCH: 240 avg steps: 4.65625 avg reward: 0.9523809552192688\n",
            "TEST EPOCH: 250 avg steps: 4.6875 avg reward: 0.9615384340286255\n",
            "TEST EPOCH: 260 avg steps: 4.6875 avg reward: 0.9259259104728699\n",
            "TEST EPOCH: 270 avg steps: 4.75 avg reward: 0.6521739363670349\n",
            "TEST EPOCH: 280 avg steps: 5.03125 avg reward: 0.9473684430122375\n",
            "TEST EPOCH: 290 avg steps: 4.9375 avg reward: 0.9545454382896423\n",
            "TEST EPOCH: 300 avg steps: 4.75 avg reward: 1.0\n",
            "TEST EPOCH: 310 avg steps: 4.875 avg reward: 1.0\n",
            "TEST EPOCH: 320 avg steps: 4.59375 avg reward: 1.0\n",
            "TEST EPOCH: 330 avg steps: 4.9375 avg reward: 0.949999988079071\n",
            "TEST EPOCH: 340 avg steps: 4.59375 avg reward: 1.0\n",
            "TEST EPOCH: 350 avg steps: 4.375 avg reward: 0.9629629850387573\n",
            "TEST EPOCH: 360 avg steps: 4.71875 avg reward: 1.0\n",
            "TEST EPOCH: 370 avg steps: 4.5 avg reward: 0.9599999785423279\n",
            "TEST EPOCH: 380 avg steps: 4.65625 avg reward: 1.0\n",
            "TEST EPOCH: 390 avg steps: 4.4375 avg reward: 1.0\n",
            "TEST EPOCH: 400 avg steps: 4.8125 avg reward: 1.0\n",
            "TEST EPOCH: 410 avg steps: 4.8125 avg reward: 1.0\n",
            "TEST EPOCH: 420 avg steps: 4.375 avg reward: 1.0\n",
            "TEST EPOCH: 430 avg steps: 5.09375 avg reward: 1.0\n",
            "TEST EPOCH: 440 avg steps: 4.6875 avg reward: 0.9473684430122375\n",
            "TEST EPOCH: 450 avg steps: 4.5 avg reward: 1.0\n",
            "TEST EPOCH: 460 avg steps: 4.71875 avg reward: 0.9130434989929199\n",
            "TEST EPOCH: 470 avg steps: 4.65625 avg reward: 1.0\n",
            "TEST EPOCH: 480 avg steps: 4.96875 avg reward: 1.0\n",
            "TEST EPOCH: 490 avg steps: 4.8125 avg reward: 1.0\n",
            "TEST EPOCH: 500 avg steps: 4.59375 avg reward: 1.0\n",
            "TEST EPOCH: 510 avg steps: 4.71875 avg reward: 1.0\n",
            "TEST EPOCH: 520 avg steps: 4.3125 avg reward: 1.0\n",
            "TEST EPOCH: 530 avg steps: 4.90625 avg reward: 1.0\n",
            "TEST EPOCH: 540 avg steps: 5.0 avg reward: 1.0\n",
            "TEST EPOCH: 550 avg steps: 4.75 avg reward: 1.0\n",
            "TEST EPOCH: 560 avg steps: 4.8125 avg reward: 0.9599999785423279\n",
            "TEST EPOCH: 570 avg steps: 4.65625 avg reward: 1.0\n",
            "TEST EPOCH: 580 avg steps: 4.5625 avg reward: 1.0\n",
            "TEST EPOCH: 590 avg steps: 4.8125 avg reward: 1.0\n",
            "TEST EPOCH: 600 avg steps: 4.65625 avg reward: 1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-b2bb1fdba156>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0marena\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mArena\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDotDic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0marena\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-01ad514ee1f4>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, agents, reset, verbose, test_callback)\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnepisodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m       \u001b[0mepisode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m       \u001b[0mnorm_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-01ad514ee1f4>\u001b[0m in \u001b[0;36mrun_episode\u001b[0;34m(self, agents, train_mode)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;31m# Get Q-values from CNet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mhidden_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0magent_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0mepisode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_records\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;31m# Pick actions based on q-values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-067bbbf9a126>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, state, messages, hidden, prev_action, agent)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mrnn_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0;31m# Produce final CNet output q-values from GRU output.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             result = _impl(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 179\u001b[0;31m                            self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             result = _impl(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "b_M3Y7DeeYbH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ACTIONS = [None, 'The prisoner decided to do nothing.', 'The prisoner chose to tell.']\n",
        "\n",
        "game.reset()\n",
        "ep = arena.run_episode(agents, False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fAqWQIFN6Hda",
        "colab_type": "code",
        "outputId": "cf9c3b4b-5cf6-4ae2-9231-95b065448b68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "cell_type": "code",
      "source": [
        "batch = 6\n",
        "game.describe_game(batch)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "has been: tensor([[0., 0., 1.],\n",
            "        [0., 1., 0.],\n",
            "        [1., 0., 0.],\n",
            "        [0., 0., 1.],\n",
            "        [0., 0., 1.],\n",
            "        [0., 0., 1.]])\n",
            "num has been: 3\n",
            "active agents:  tensor([3, 2, 1, 3, 3, 3])\n",
            "reward: tensor([1., 1., 1.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0RtSodjd5-xH",
        "colab_type": "code",
        "outputId": "15d1774a-ae66-4870-e281-1fe2bead351e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "cell_type": "code",
      "source": [
        "for i, step in enumerate(ep.step_records[:-1]):\n",
        "    print('Day', i + 1)\n",
        "    active_agent = game.active_agent[batch][i].item()\n",
        "    print('Prisoner Selected for the interrogation: ', active_agent)\n",
        "    print(ACTIONS[step.a_t[batch].detach().numpy()[active_agent - 1]])\n",
        "    if step.comm[batch].detach().numpy()[active_agent - 1][0] == 1.0:\n",
        "        print('The prisoner toggled the light bulb.')\n",
        "    print()\n",
        "    if step.terminal[batch]:\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Day 1\n",
            "Prisoner Selected for the interrogation:  3\n",
            "The prisoner decided to do nothing.\n",
            "\n",
            "Day 2\n",
            "Prisoner Selected for the interrogation:  2\n",
            "The prisoner decided to do nothing.\n",
            "\n",
            "Day 3\n",
            "Prisoner Selected for the interrogation:  1\n",
            "The prisoner chose to tell.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ao5tByOJ6Mew",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}